{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download dataset from Zenodo, unzip and format into folders"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "asJgSN12CYcS",
    "colab": {}
   },
   "source": [
    "!wget -O FSDKaggle2018.audio_train.zip https://zenodo.org/record/2552860/files/FSDKaggle2018.audio_train.zip?download=1\n",
    "!wget -O FSDKaggle2018.audio_test.zip https://zenodo.org/record/2552860/files/FSDKaggle2018.audio_test.zip?download=1\n",
    "!wget -O FSDKaggle2018.meta.zip https://zenodo.org/record/2552860/files/FSDKaggle2018.meta.zip?download=1\n",
    "!unzip \\*.zip\n",
    "!mkdir data submission figures\n",
    "%mv FSDKaggle2018.audio_train data/train\n",
    "%mv FSDKaggle2018.audio_test data/test\n",
    "%mv -v FSDKaggle2018.meta/* submission/\n",
    "%cd data/train/\n",
    "!ls -F |grep -v / | wc -l\n",
    "%cd ../test/\n",
    "!ls -F |grep -v / | wc -l\n",
    "%cd ../..\n",
    "!rm FSDKaggle2018.audio_train.zip FSDKaggle2018.audio_test.zip FSDKaggle2018.meta.zip\n",
    "!rm -R FSDKaggle2018.meta\n",
    "!pip install -r requirements.txt"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "kyMYcIS2DsMT",
    "colab": {}
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import cm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tqdm import tqdm\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22gUDR5JJwHg",
    "colab_type": "text"
   },
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wys0orCPJutP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "df_train_og = train = pd.read_csv(\"submission/train_post_competition.csv\")\n",
    "\n",
    "df_grouped = df_train_og.groupby(['label', 'manually_verified']).count().drop(['freesound_id','license'], axis = 1)\n",
    "\n",
    "cmap = cm.get_cmap('viridis')\n",
    "\n",
    "plot = df_grouped.unstack().reindex(df_grouped.unstack().sum(axis=1).sort_values().index)\\\n",
    "          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,9), colormap=cmap)\n",
    "\n",
    "plot.legend( ['Unverified', 'Verified']);\n",
    "plot.set_xlabel(\"Category\")\n",
    "plot.set_ylabel(\"No. of Samples\")\n",
    "\n",
    "plot.figure.savefig(\"figures/Category_vs_No.samples.png\", bbox_inches=\"tight\", dpi=100)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmPL-tToGcZP"
   },
   "source": [
    "## 2. Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AuB3dgYQGcyq",
    "outputId": "85a5b547-ce0d-4d2d-e960-a13e45c6f1c9"
   },
   "outputs": [],
   "source": [
    "\"\"\"Pre-process\"\"\"\n",
    "\n",
    "sampling_rate = 44100\n",
    "\n",
    "sample_submission = pd.read_csv(\"submission/\" + \"test_post_competition_scoring_clips.csv\")\n",
    "sample_submission = sample_submission[['fname']].copy()\n",
    "\n",
    "# Ignoring the empty wavs\n",
    "sample_submission['toremove'] = 0\n",
    "sample_submission.loc[sample_submission.fname.isin([\n",
    "    '0b0427e2.wav', '6ea0099f.wav', 'b39975f5.wav'\n",
    "]), 'toremove'] = 1\n",
    "\n",
    "print('Train...')\n",
    "os.makedirs('data/audio_train_trim/', exist_ok=True)\n",
    "for filename in tqdm(train.fname.values):\n",
    "    x, sr = librosa.load('data/train/' + filename, sampling_rate)\n",
    "    x = librosa.effects.trim(x)[0]\n",
    "    np.save('data/audio_train_trim/' + filename + '.npy', x)\n",
    "\n",
    "print('Test...')\n",
    "os.makedirs('data/audio_test_trim/', exist_ok=True)\n",
    "for filename in tqdm(sample_submission.loc[lambda x: x.toremove == 0, :].fname.values):\n",
    "    x, sr = librosa.load('data/test/' + filename, sampling_rate)\n",
    "    x = librosa.effects.trim(x)[0]\n",
    "    np.save('data/audio_test_trim/' + filename + '.npy', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_8zbdA1HvDz"
   },
   "source": [
    "## 3. Compute Log-Mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LY-3yu4V-rRm",
    "outputId": "473eb9e9-3e4b-41d7-9592-6a1c061ea6b1"
   },
   "outputs": [],
   "source": [
    "\"\"\"Compute Log Mel-Spectrograms\"\"\"\n",
    "# calculate log mel: https://datascience.stackexchange.com/questions/27634/how-to-convert-a-mel-spectrogram-to-log-scaled-mel-spectrogram\n",
    "# Paper: https://arxiv.org/pdf/1608.04363.pdf\n",
    "\n",
    "def compute_melspec(filename, indir, outdir):\n",
    "    wav = np.load(indir + filename + '.npy')\n",
    "    wav = librosa.resample(wav, 44100, 22050)\n",
    "    melspec = librosa.feature.melspectrogram(wav,\n",
    "                                             sr=22050,\n",
    "                                             n_fft=1764,\n",
    "                                             hop_length=220,\n",
    "                                             n_mels=64)\n",
    "    logmel = librosa.core.power_to_db(melspec)\n",
    "    np.save(outdir + filename + '.npy', logmel)\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "os.makedirs('data/mel_spec_train', exist_ok=True)\n",
    "for x in tqdm(train.fname.values):\n",
    "    compute_melspec(x, 'data/audio_train_trim/', 'data/mel_spec_train/')\n",
    "\n",
    "\n",
    "os.makedirs('data/mel_spec_test/', exist_ok=True)\n",
    "for x in tqdm(sample_submission.loc[lambda x: x.toremove == 0, :].fname.values):\n",
    "    compute_melspec(x, 'data/audio_test_trim/', 'data/mel_spec_test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkjitSKGHzB0"
   },
   "source": [
    "## 4. Compute Summary metrics of various spectral and time based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "colab_type": "code",
    "id": "j4wEuLyhEWXo",
    "outputId": "9a53515d-987b-4b1d-ac56-aa9b34fa8bd8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# number of cores to use (-1 uses all)\n",
    "num_cores = -1\n",
    "\n",
    "print('Train...')\n",
    "train = df_train_og\n",
    "\n",
    "train_feats = Parallel(n_jobs=num_cores)(\n",
    "    delayed(all_feats)('data/audio_train_trim/' + x + '.npy')\n",
    "    for x in tqdm(train.fname.values))\n",
    "\n",
    "train_feats_df = pd.DataFrame(np.vstack(train_feats))\n",
    "train_feats_df['fname'] = pd.Series(train.fname.values, index=train_feats_df.index)\n",
    "train_feats_df.to_pickle('data/train_tab_feats.pkl')\n",
    "\n",
    "\n",
    "print('Test...')\n",
    "\n",
    "test_feats = Parallel(n_jobs=num_cores)(\n",
    "    delayed(all_feats)('data/audio_test_trim/' + x + '.npy')\n",
    "    for x in tqdm(sample_submission\n",
    "                  .loc[lambda x: x.toremove == 0, :]\n",
    "                  .fname.values))\n",
    "\n",
    "test_feats_df = pd.DataFrame(np.vstack(test_feats))\n",
    "test_feats_df['fname'] = pd.Series(sample_submission.loc[lambda x: x.toremove == 0, :].fname.values,\n",
    "                                   index=test_feats_df.index)\n",
    "test_feats_df.to_pickle('data/test_tab_feats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VysKVEOwCYch"
   },
   "source": [
    "## 5. Get files from original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpuLfufbCYch"
   },
   "outputs": [],
   "source": [
    "file_to_tag = pd.Series(df_train_og['label'].values,index=df_train_og['fname']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fp4MP8HJCYck"
   },
   "outputs": [],
   "source": [
    "def getTag(x):\n",
    "    return (file_to_tag[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_CNzBAECYcp"
   },
   "source": [
    "## 6. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWRW9YbpCYcp"
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"data/train_tab_feats.pkl\",\"rb\")\n",
    "df_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data/test_tab_feats.pkl\",\"rb\")\n",
    "df_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHE2YjvHCYcr"
   },
   "outputs": [],
   "source": [
    "total = pd.concat([df_train,df_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YuopDgACYcu"
   },
   "source": [
    "#### Need usable test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xF_UMdbSCYcz"
   },
   "outputs": [],
   "source": [
    "df_train['tag'] = df_train['fname'].apply(getTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfnQaiiKCYc1"
   },
   "outputs": [],
   "source": [
    "df_train_copy = df_train.drop(['fname','tag'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2D62t_9CYc5"
   },
   "source": [
    "## 7. Reduce Dimensions and Make Train Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8As13FIsCYc5"
   },
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "X = LDA.fit_transform(df_train_copy, df_train['tag'])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,  df_train['tag'], shuffle = True, test_size = 0.2, random_state = 42)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_val = encoder.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSmNEg4JCYc_"
   },
   "source": [
    "## 8. SGD Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2936OFjzCYc_"
   },
   "outputs": [],
   "source": [
    "SGD = SGDClassifier()\n",
    "SGD.fit(x_train,y_train)\n",
    "y_pred = SGD.predict(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI55NQqQCYdB"
   },
   "outputs": [],
   "source": [
    "new_index = list(encoder.classes_)\n",
    "new_index.append('accuracy')\n",
    "new_index.append('macro avg')\n",
    "new_index.append('weighted avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuS0JIdnCYdE"
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "df_sgd_first = pd.DataFrame(report).transpose()\n",
    "df_sgd_first.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "duYnVhVRCYdG",
    "outputId": "2f6781f6-c2f3-42a8-c805-ab4a7743d0d8"
   },
   "outputs": [],
   "source": [
    "df_sgd_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IACqudVfCYdL"
   },
   "source": [
    "## 9. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTEupAHuCYdL"
   },
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(x_train,y_train)\n",
    "y_pred = svm.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9KGM-kqCYdO"
   },
   "outputs": [],
   "source": [
    "new_index = list(encoder.classes_)\n",
    "new_index.append('accuracy')\n",
    "new_index.append('macro avg')\n",
    "new_index.append('weighted avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6TMhnifCYdP"
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "df_svm_first = pd.DataFrame(report).transpose()\n",
    "df_svm_first.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wT6SGgYWCYdR",
    "outputId": "7ca10d06-9ac3-45ef-8db9-b9f8a5f89bdc"
   },
   "outputs": [],
   "source": [
    "df_svm_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6twJQgDCYdT"
   },
   "source": [
    "## 10. Grid Search On SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drYzDZCDCYdT"
   },
   "outputs": [],
   "source": [
    "def SVC_GridSearch(X, Y, X_test, Y_test):\n",
    "    svc = SVC()\n",
    "    parameters = {\n",
    "        'C': (0.5,1,2),\n",
    "        'kernel': ('rbf','linear','poly', 'sigmoid'),\n",
    "        'shrinking': (True, False),\n",
    "        'decision_function_shape': ('ovp','ovr'),\n",
    "        \n",
    "\n",
    "    }\n",
    "    grid_search = GridSearchCV(svc, parameters, n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X, Y)\n",
    "    accuracy = grid_search.best_score_\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    classifier = grid_search.best_estimator_\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_pred, Y_test)\n",
    "    return best_parameters, accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bU_51YIICYdV"
   },
   "outputs": [],
   "source": [
    "best_parameters, accuracy, test_accuracy = SVC_GridSearch(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oUgciu5PCYdX",
    "outputId": "63d0eaa1-0e7b-4026-a1c8-e391f537358d"
   },
   "outputs": [],
   "source": [
    "bestSVM = SVC()\n",
    "bestSVM.set_params(**best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38eBussyCYdY"
   },
   "outputs": [],
   "source": [
    "bestSVM.fit(x_train,y_train)\n",
    "y_pred = bestSVM.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GE58l6lyCYdb"
   },
   "outputs": [],
   "source": [
    "new_index = list(encoder.classes_)\n",
    "new_index.append('accuracy')\n",
    "new_index.append('macro avg')\n",
    "new_index.append('weighted avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygvq84mcCYdc"
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "df_svm = pd.DataFrame(report).transpose()\n",
    "df_svm.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uRxxgboACYde",
    "outputId": "cb2a6824-777e-4475-be2d-11736f0defc9"
   },
   "outputs": [],
   "source": [
    "df_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUaXpVRhCYdf"
   },
   "source": [
    "## 11. Vanilla Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "id": "qjAjG3ZlCYdf",
    "outputId": "2a12afd8-1b3f-4846-c777-08aa03122219"
   },
   "outputs": [],
   "source": [
    "# Set the input and output sizes\n",
    "input_size = 40 #NUMBER INPUTS HERE#\n",
    "output_size = 41 #NUMBER OUTPUTS HERE#\n",
    "\n",
    "\n",
    "#DEFINE HIDDEN LAYER SIZE\n",
    "#CAN HAVE MULTIPLE DIFFERENT SIZED LAYERS IF NEEDED\n",
    "#50 NICE START POINT FOR BEING TIME EFFICIENT BUT STILL RELATIVELY COMPLEX\n",
    "hidden_layer_size = 100\n",
    "  \n",
    "\n",
    "#MODEL SPECIFICATIONS\n",
    "model = Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    Dropout(0.2),\n",
    "    Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n",
    "    Dropout(0.2),\n",
    "\n",
    "\n",
    "    # POTENTIALLY MULTIPLE MORE LAYERS HERE #\n",
    "    # NO SINGLE ACTIVATION NECESSARILY BEST (AT THIS STAGE I DO NOT FULLY UNDERSTAND DIFFERENCES, TRY DIFFERENT VARIATIONs)\n",
    "    \n",
    "    # FINAL LAYER MUST TAKE OUTPUT SIZE\n",
    "    #FOR CLASSIFICATION PROBLEMS USE SOFTMAX AS ACTIVATION\n",
    "    Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "#COMPILE MODEL GIVING IT OPTIMIZER LOSS FUNCTION AND METRIC OF INTEREST\n",
    "# MOST TIMES USE ADAM FOR OPTIMIZER (LOOK AT OTHERS THOUGH) \n",
    "# lOSS FUNCTION - MANY DIFFERENT VARIATIONS sparse_categorical_crossentropy IS BASICALLY MIN SUM OF SQUARES\n",
    "# TO NOW I AM ONLY INTERESTED IN ACCURACY AT EACH LEVEL (HAVE NOT LOOKED AT OTHER OPTIONS`)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "###                            ### \n",
    "###                            ###\n",
    "###          TRAINING          ###\n",
    "###                            ###\n",
    "###                            ###\n",
    "\n",
    "# SET SIZE OF BATCHES (FOR SHUFFLING IN PARTS WHEN OVERALL SIZE TO BIG)\n",
    "batch_size = 128\n",
    "\n",
    "# SET MAXIMUM NUMBER OF EPOCHS (JUST SO DOESNT RUN ENDLESSLY)\n",
    "max_epochs = 100\n",
    "\n",
    "# SET EARLY STOPPING FUNCTION\n",
    "# PATIENCE EQUAL 0 (DEFAULT) => STOPS AS SOON AS FOLLOWING EPOCH HAS REDUCED LOSS\n",
    "# PATIENCE EQUAL N => STOPS AFTER N SUBSEQUENT INCREASING LOSSES\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "\n",
    "###                            ### \n",
    "###                            ###\n",
    "###         FIT MODEL          ###\n",
    "###                            ###\n",
    "###                            ###\n",
    "\n",
    "model.fit(x_train, # train inputs\n",
    "          y_train, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(x_val, y_val), # validation data\n",
    "          verbose = 1 # shows some information for each epoch so we can analyse\n",
    "          )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "1HxMPMqRCYdj",
    "outputId": "9dd77a97-ca1b-4921-a135-a0e25decb8f8"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "vanilla_nn = classification_report(y_val, y_pred)\n",
    "print(vanilla_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPnnNPGeP3m1"
   },
   "source": [
    "##  summary_feats_funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lgo9oYkCYdn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "sr = 44100\n",
    "\n",
    "\n",
    "def compute_summ_features(x):\n",
    "    ans = np.hstack((\n",
    "        np.mean(x, axis=1),\n",
    "        np.std(x, axis=1),\n",
    "        skew(x, axis=1),\n",
    "        np.max(x, axis=1),\n",
    "        np.min(x, axis=1)))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def feat_set_1(x, stft):\n",
    "    # Features mentioned in\n",
    "    # - http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/\n",
    "    # - https://www.kaggle.com/amlanpraharaj/xgb-using-mfcc-opanichev-s-features-lb-0-811\n",
    "\n",
    "    # Mel-scaled power spectrogram\n",
    "    mels = librosa.feature.melspectrogram(x, sr=sr, S=stft)\n",
    "\n",
    "    # Mel-frequency cepstral coefficients\n",
    "    mfccs = librosa.feature.mfcc(y=x, sr=sr, S=stft, n_mfcc=40)\n",
    "\n",
    "    # chorma-stft: Compute a chromagram from a waveform or power spectrogram\n",
    "    chromas = librosa.feature.chroma_stft(S=stft, sr=sr)\n",
    "\n",
    "    # spectral_contrast: Compute spectral contrast\n",
    "    contrasts = librosa.feature.spectral_contrast(x, S=stft, sr=sr)\n",
    "\n",
    "    # Compute roll-off frequency\n",
    "    rolloffs = librosa.feature.spectral_rolloff(x, sr=sr, S=stft)\n",
    "\n",
    "    # Compute the spectral centroid\n",
    "    scentroids = librosa.feature.spectral_centroid(x, sr=sr, S=stft)\n",
    "\n",
    "    # Compute p’th-order spectral bandwidth\n",
    "    bandwidths = librosa.feature.spectral_bandwidth(x, sr=sr, S=stft)\n",
    "\n",
    "    # tonnetz: Computes the tonal centroid features (tonnetz)\n",
    "    tonnetzs = librosa.feature.tonnetz(y=librosa.effects.harmonic(x), sr=sr)\n",
    "\n",
    "    # zero crossing rate\n",
    "    zero_crossing_rates = librosa.feature.zero_crossing_rate(x)\n",
    "\n",
    "    tmp = (mels, mfccs, chromas, contrasts,\n",
    "           rolloffs, scentroids, bandwidths,\n",
    "           tonnetzs, zero_crossing_rates)\n",
    "\n",
    "    ans = np.hstack([\n",
    "        compute_summ_features(x)\n",
    "        for x in tmp\n",
    "    ])\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "# Features from https://www.kaggle.com/opanichev/lightgbm-baseline\n",
    "def calc_part_features(data, n=2):\n",
    "    ans = []\n",
    "    for j, i in enumerate(range(0, len(data), len(data)//n)):\n",
    "        if j == (n-1):\n",
    "            i = len(data) - 1\n",
    "        if j < n:\n",
    "            ans.append(np.mean(data[i:i + len(data)//n]))\n",
    "            ans.append(np.std(data[i:i + len(data)//n]))\n",
    "            ans.append(np.min(data[i:i + len(data)//n]))\n",
    "            ans.append(np.max(data[i:i + len(data)//n]))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def feat_set_4(x):\n",
    "    abs_data = np.abs(x)\n",
    "    diff_data = np.diff(x)\n",
    "\n",
    "    ans = []\n",
    "\n",
    "    n = 1\n",
    "    ans += calc_part_features(x, n=n)\n",
    "    ans += calc_part_features(abs_data, n=n)\n",
    "    ans += calc_part_features(diff_data, n=n)\n",
    "\n",
    "    n = 2\n",
    "    ans += calc_part_features(x, n=n)\n",
    "    ans += calc_part_features(abs_data, n=n)\n",
    "    ans += calc_part_features(diff_data, n=n)\n",
    "\n",
    "    n = 3\n",
    "    ans += calc_part_features(x, n=n)\n",
    "    ans += calc_part_features(abs_data, n=n)\n",
    "    ans += calc_part_features(diff_data, n=n)\n",
    "\n",
    "    return np.array(ans)\n",
    "\n",
    "\n",
    "# Features from https://www.kaggle.com/agehsbarg/audio-challenge-cnn-with-concatenated-inputs\n",
    "def get_spectra_win(y, L, N):\n",
    "    dft = np.fft.fft(y)\n",
    "    fl = np.abs(dft)\n",
    "    xf = np.arange(0.0, N/L, 1/L)\n",
    "    return (xf, fl)\n",
    "\n",
    "\n",
    "def get_spectra(signal, fs, M=1000, sM=500):\n",
    "\n",
    "    N = signal.shape[0]\n",
    "    ind = np.arange(100, N, M)\n",
    "\n",
    "    spectra = []\n",
    "    meanspectrum = np.repeat(0, M)\n",
    "\n",
    "    for k in range(1, len(ind)):\n",
    "        n1 = ind[k-1]\n",
    "        n2 = ind[k]\n",
    "        y = signal[n1:n2]\n",
    "        L = (n2-n1)/fs\n",
    "        N = n2-n1\n",
    "        (xq, fq) = get_spectra_win(y, L, N)\n",
    "        spectra.append(fq)\n",
    "\n",
    "    spectra = pd.DataFrame(spectra)\n",
    "    meanspectrum = spectra.apply(lambda x: np.log(1+np.mean(x)), axis=0)\n",
    "    stdspectrum = spectra.apply(lambda x: np.log(1+np.std(x)), axis=0)\n",
    "\n",
    "    meanspectrum = meanspectrum[0:sM]\n",
    "    stdspectrum = stdspectrum[0:sM]\n",
    "\n",
    "    return (meanspectrum, stdspectrum)\n",
    "\n",
    "\n",
    "def get_width(w):\n",
    "    if np.sum(w) == 0:\n",
    "        return [0, 0, 0]\n",
    "    else:\n",
    "        z = np.diff(np.where(np.insert(np.append(w, 0), 0, 0) == 0))-1\n",
    "        z = z[z > 0]\n",
    "    return [np.log(1+np.mean(z)),\n",
    "            np.log(1+np.std(z)),\n",
    "            np.log(1+np.max(z)),\n",
    "            len(z)]\n",
    "\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "# predictors related to peaks\n",
    "def num_peaks(x):\n",
    "    x = np.array(x[0:len(x)])\n",
    "    n10 = np.sum(x > 0.10*np.max(x))\n",
    "    n20 = np.sum(x > 0.20*np.max(x))\n",
    "    n50 = np.sum(x > 0.50*np.max(x))\n",
    "    n90 = np.sum(x > 0.90*np.max(x))\n",
    "    n99 = np.sum(x > 0.99*np.max(x))\n",
    "    lead_min = np.min(np.where(x == np.max(x)))\n",
    "    w10 = get_width(1*(x > 0.10*np.max(x)))\n",
    "    w20 = get_width(1*(x > 0.20*np.max(x)))\n",
    "    w50 = get_width(1*(x > 0.50*np.max(x)))\n",
    "    w90 = get_width(1*(x > 0.90*np.max(x)))\n",
    "    w99 = get_width(1*(x > 0.99*np.max(x)))\n",
    "    W = w10+w20+w50+w90+w99\n",
    "\n",
    "    f_sc = np.sum(np.arange(0, len(x))*(x*x)/np.sum(x*x))\n",
    "\n",
    "    i1 = np.where(x < 0.10*np.max(x))[0]\n",
    "    if i1.size == 0:\n",
    "        lincoef_w = [0, 0, 0]\n",
    "    else:\n",
    "        a1 = i1[i1 < lead_min]\n",
    "        a2 = i1[i1 > lead_min]\n",
    "\n",
    "        if a1.size == 0:\n",
    "            i1_left = 0\n",
    "        else:\n",
    "            i1_left = np.max(i1[i1 < lead_min])\n",
    "        if a2.size == 0:\n",
    "            i1_right = 0\n",
    "        else:\n",
    "            i1_right = np.min(i1[i1 > lead_min])\n",
    "\n",
    "        lead_min_width = i1_right - i1_left\n",
    "        if (lead_min_width > 2):\n",
    "            poly_w = PolynomialFeatures(degree=2, include_bias=False)\n",
    "            f_ind_w = poly_w.fit_transform(\n",
    "                np.arange(i1_left, i1_right, 1).reshape(-1, 1))\n",
    "            clf_w = linear_model.LinearRegression()\n",
    "            linmodel_w = clf_w.fit(f_ind_w, np.array(x[i1_left:i1_right]))\n",
    "            lincoef_w = list(linmodel_w.coef_)+[linmodel_w.intercept_]\n",
    "        else:\n",
    "            lincoef_w = [0, 0, 0]\n",
    "\n",
    "    S = np.sum(x)\n",
    "    S_n = np.sum(x)/len(x)\n",
    "    S2 = np.sqrt(np.sum(x*x))\n",
    "    S2_n = np.sqrt(np.sum(x*x))/len(x)\n",
    "    integrals = [S, S_n, S2, S2_n]\n",
    "\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    f_ind = poly.fit_transform(np.arange(0, len(x)).reshape(-1, 1))\n",
    "    clf = linear_model.LinearRegression()\n",
    "    linmodel = clf.fit(f_ind, x)\n",
    "    lincoef_spectrum = list(linmodel.coef_)+[linmodel.intercept_]\n",
    "\n",
    "    high_freq_sum_50 = np.sum(x[0:50] >= 0.5*np.max(x))\n",
    "    high_freq_sum_90 = np.sum(x[0:50] >= 0.9*np.max(x))\n",
    "\n",
    "    r = [f_sc, n10, n20, n50, n90, n99,\n",
    "         lead_min, high_freq_sum_50, high_freq_sum_90] \\\n",
    "        + W + lincoef_spectrum + integrals + lincoef_w\n",
    "    return r\n",
    "\n",
    "\n",
    "def runningMeanFast(x, N=20):\n",
    "    return np.convolve(x, np.ones((N,))/N)[(N-1):]\n",
    "\n",
    "\n",
    "def feat_set_2(x):\n",
    "    rawsignal = x\n",
    "    rawsignal_sq = rawsignal*rawsignal\n",
    "    silenced = []\n",
    "    sound = []\n",
    "    attack = []\n",
    "    for wd in [2000]:\n",
    "        rawsignal_sq_rm = running_mean(rawsignal_sq, wd)\n",
    "        w1 = 1*(rawsignal_sq_rm < 0.01*np.max(rawsignal_sq_rm))\n",
    "        silenced = silenced + get_width(w1)\n",
    "        w2 = 1*(rawsignal_sq_rm < 0.05*np.max(rawsignal_sq_rm))\n",
    "        silenced = silenced + get_width(w2)\n",
    "        w3 = 1*(rawsignal_sq_rm > 0.05*np.max(rawsignal_sq_rm))\n",
    "        sound = sound + get_width(w3)\n",
    "        w4 = 1*(rawsignal_sq_rm > 0.25*np.max(rawsignal_sq_rm))\n",
    "        sound = sound + get_width(w4)\n",
    "        time_to_attack = np.min(np.where(\n",
    "            rawsignal_sq_rm > 0.99*np.max(rawsignal_sq_rm)))\n",
    "        time_rel = np.where(rawsignal_sq_rm < 0.2*np.max(rawsignal_sq_rm))[0]\n",
    "        if (time_rel.size == 0):\n",
    "            time_to_relax = len(rawsignal_sq_rm)\n",
    "        elif (time_rel[time_rel > time_to_attack].size == 0):\n",
    "            time_to_relax = len(rawsignal_sq_rm)\n",
    "        else:\n",
    "            time_to_relax = np.min(time_rel[time_rel > time_to_attack])\n",
    "        attack.append(np.log(1+time_to_attack))\n",
    "        attack.append(np.log(1+time_to_relax))\n",
    "\n",
    "    lr = len(rawsignal)\n",
    "    zerocross_tot = np.log(\n",
    "        1 + np.sum(\n",
    "            np.array(\n",
    "                rawsignal[0:(lr-1)]\n",
    "            ) * np.array(rawsignal[1:lr]) <= 0))\n",
    "    zerocross_prop = np.sum(\n",
    "        np.array(\n",
    "            rawsignal[0:(lr-1)]) * np.array(rawsignal[1:lr]) <= 0) / lr\n",
    "    return np.array(sound + attack + [zerocross_tot, zerocross_prop])\n",
    "\n",
    "\n",
    "def feat_set_3(x):\n",
    "    (m, sd) = get_spectra(x, sr, 2000, 1000)\n",
    "    ans1 = np.array(num_peaks(m))\n",
    "    ans2 = (lambda x: x[np.arange(0, len(x), 40)])(np.array(runningMeanFast(m)))\n",
    "    return np.concatenate((ans1, ans2))\n",
    "\n",
    "\n",
    "def all_feats(filename):\n",
    "    x = np.load(filename)\n",
    "    stft = np.abs(librosa.stft(x))\n",
    "    out1 = feat_set_1(x, stft=stft)\n",
    "    out2 = feat_set_2(x)\n",
    "    out3 = feat_set_3(x)\n",
    "    out4 = feat_set_4(x)\n",
    "\n",
    "    assert out1.shape[0] == 985\n",
    "    assert out2.shape[0] == 12\n",
    "    assert out3.shape[0] == 64\n",
    "    assert out4.shape[0] == 72\n",
    "\n",
    "    return np.concatenate((\n",
    "        out1, out2, out3, out4\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "freesound_audio_tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
